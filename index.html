<!DOCTYPE html>
<html>
<head>
    <title>Govind Ramesh</title>
    <link rel="stylesheet" href="gistStyle.css">
    <style>
        .title a {
            display: block;
            font-size:40px;
            padding: 10px 0px 0px 100px;
	          background-color: grey;
            color:black;
            font-weight:bold;
            text-align: left;
            text-decoration: none;
        }
        .subtitle a {
            display: block;
            font-size:17px;
	          padding: 0px 0px 10px 100px;
            text-align: left;
            text-decoration: none;
        }
        body {
            background-color: grey;
            margin: 0 auto;
            background-position:center;
            background-size: contain;
        }

        .menu {
            position: sticky;
            top: 0;
            background-color: black;
            padding:0px 0px 0px 84px;
            color:white;
            z-index: 100;
        }
        .menu a {
            display:inline-block;
            font-size: 20px;
            color: white;
            text-align: center;
            padding: 10px 16px;
            text-decoration: none;
        }
        .dropdown {
            display:inline-block;
        }
        .dropdown .dropbtn {
            font-size: 20px;
            border: none;
            outline: none;
            color: white;
            padding: 10px 16px;
            background-color: inherit;
            font-family: inherit;
            margin: 0;
        }
        .menu a:hover, .dropdown:hover .dropbtn {
            background-color: #595959;
        }
        .dropdown-content {
            display: none;
            position:absolute;
            background-color: black;
            width: fit-content;
            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
            z-index: 1;
        }
        .dropdown-content a {
            float: none;
            color: white;
            padding: 12px 16px;
            text-decoration: none;
            display: block;
            text-align: left;
        }
        .dropdown-content a:hover {
            background-color: #909090;
        }
        .dropdown:hover .dropdown-content {
            display: block ;
        }
        footer {
            width: 100%;
            bottom: 0px;
            background-color: black;
            color: white;
            position: fixed;
	          padding:10px 0px 10px 0px;
            text-align:center;
            font-size:17px;
            font-weight:bold;
        }
        .body_sec {
            background-color: white;
            margin-left:180px;
            margin-right:180px;
            padding-top:40px;
            padding-right:20px;
            padding-left:20px;
            padding-bottom:60px;
            font-size:17px;
        }

    </style>
</head>

<body>

    <!-- Header Section -->
    <header>
        <div class = "title"><a href="https://govindramesh.github.io">Govind Ramesh</a></title>
        <div class="subtitle"><a href="https://govindramesh.github.io">GitHub Page for Programming Projects</a></div>
    </header>

    <!-- Menu Navigation Bar -->
    <div class="menu">
        <a href="https://govindramesh.github.io">HOME</a>
        <div class="dropdown">
            <a class="dropbtn">PD-3DCNN</a>
            <div class="dropdown-content">
                <a target="_blank" rel="noopener noreferrer" href="Research Paper.pdf">Research Paper</a>
                <a href="https://govindramesh.github.io/PD-3DCNN/">3D CNN Tutorial</a>
            </div>
        </div>
        <a href="https://govindramesh.github.io/PoliceBrutality/">Police Brutality</a>
    </div>

    <!-- Body section -->
    <div class = "body_sec">
      <h1 style="line-height: 70%">3D Convolutional Neural Network in Keras</h1>
      <p style="font-size:19px"><i>In this post, we will be going over what a 3D Convolutional Neural Network is, how it works, and how to implement one using Keras.
      </i></p1>
      <section id="Contents">
           <p><br><span style="font-size:18px; font-weight:bold;">Sections:</span>
           <br><a href="#sec1">What is a Convolutional Neural Network?</a>
           <br><a href="#sec2">Convolutional Neural Network Architecture</a>
           <br><a href="#sec3">Preprocessing</a>
           <br><a href="#sec4">3D Convolutional Neural Network Code</a>
           <br></p>
      </section>
      <hr>
      <section style="padding-top:40px; margin-top:-50px;" id="sec1">
          <h3>What is a Convolutional Neural Network?</h3>
          <p>Convolutional Neural Networks (CNNs) are a class of Artificial Neural Networks (ANNs) primarily used for problems involving image classification. Rather than using each pixel (or voxel in 3D) value as a feature as would happen with a standard fully-connected multilayer perceptron, CNNs connect each neuron to a small section of the prior layer, drastically reducing the number of parameters and, as a result, the amount of overfitting and computational power. This process is done through the use of learnable filters and convolutions, hence the name CNN.
          </p>
          <p>Convolutional Neural Networks have become increasingly popular in the medical field due to their effectiveness in image analysis. They have been used throughout literature for tasks such as image segmentation, disease classification, abnormality detection, and computer aided diagnosis. For our purposes, we will be using a 3D Convolutional Neural Network to diagnose Parkinson’s Disease (PD) using 871 DaTscan SPECT brain images from the PPMI database.
          </p>
      </section>
      <hr>
      <section style="padding-top:40px; margin-top:-50px;" id="sec2">
          <h3>Convolutional Neural Network Architecture</h3>
          <p>A standard Convolutional Neural Network architecture consists of convolutional layers, pooling layers, and dense layers. In general, the input images go through a series of convolutional layers and a pooling of the activations, followed by dense layers which make the classification. There are many variations of CNNs currently in practice such as AlexNet, ResNet, GoogLeNet, and VGGNet. The learning process is done through backpropagation, in which the gradient of the error function is calculated with respect to the network’s weights. Through gradient descent, the error is minimized by updating the weights.
          </p>
          <h4>3D Convolutional Layer</h4>
          <p>The convolutional layer is central to the function of CNNs, using a set of learnable filters that function similar to weights in a multilayer perceptron. These filters are relatively small compared to the image. During the forward pass, each filter shifts, or convolves, along the dimensions of the input image, computing dot products in which each element in the filter is multiplied by its respective value in the filter-sized patch of the input image and summed. These dot products are arranged spatially, producing an activation map of the image showing where the filter was detected in the input image. Through this process, convolutional layers introduce translation invariance, since the shifting of the filter across the whole input enables the identification of that feature anywhere in the image. The values within each filter are learned and updated during backpropagation with gradient descent to minimize the loss.
          </p>
          <p>In a 2D convolutional layer, the activation maps produced are 2D, but are computed multiple times for the total number of filters in the layer (a hyperparameter) and stacked to create a 3D output volume. Therefore, extending to a 3D convolutional layer, a number of 3D filters convolve along the dimensions of the input volume, creating 3D activation maps which are stacked to create a 4D volume, in which the 4th dimension is the number of filters in the layer.
          </p>
          <p>Convolutional layers can be applied to not only the input image, but also the outputs of other convolutional layers (following activation and pooling operations). By stacking multiple convolutional layers, deeper layers are able to detect more complex features. Initial convolutional layers can learn simple features such as lines from the input image, while subsequent layers can extract combinations of these simple features to construct more complex filters specific to the task.
          </p>
          <p>Convolutional layers use four main hyperparameters: filter size, number of filters (<i>n</i>), stride, and zero-padding. Filter size in a 2D CNN is generally set to be a square with size of 3, 5, or 7. For our purposes, we will use filter dimensions of 3 &times; 3 &times; 3 for each 3D convolutional layer. We use three convolutional layers, progressively increasing the number of filters for each subsequent layer, resulting in <i>n</i><sub>1</sub> &equals; 32, <i>n</i><sub>2</sub> &equals; 64, and <i>n</i><sub>3</sub> &equals; 128 filters. Stride determines how many pixels or voxels the filter shifts over for each convolution, and is usually set to 1 since any higher value reduces the amount of overlap and downsamples the input, an operation that is already performed with pooling. Zero-padding adds a border of zeros around the input, enabling the voxels on the edges to be in the center of the filter during convolution and creating an output activation map with the same dimensions as the input. We use zero-padding in each convolutional layer.
          </p>
          <h4>Activation</h4>
          <p>Activation functions are applied to every output function in ANNs. Activations enable the model to learn complex patterns in multidimensional data by introducing non-linearity. There are many activation functions, but the most popular in recent years is the Rectified Linear Unit (ReLU), especially for CNNs. ReLU functions avoid the problem of saturation seen with Tanh and Sigmoid activations, which effectively kills the gradient and the learning process. Hence, ReLU has been shown to converge six times faster than other activation functions. ReLU is computed by the function <p align = center><a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=max(0,x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=max(0,x)" title="f(x)=max(0,x)" />
          </a></p> which proves to be computationally efficient during backpropagation compared to other functions using expensive operations. In this study, we will use the ReLU activation function with every convolutional and dense layer.
          </p>
          <h4>Max Pooling</h4>
          <p>Pooling layers serve as a downsampling technique used between convolutional layers. Pooling reduces the amount of parameters which decreases the amount of computation power needed and limits overfitting to the data. In practice, max pooling layers are used, in which the max operation is applied to the pool. Traditional max pooling layers use filters/pool size of dimensions 2 &times; 2 and a stride of 2 for the downsampling operation, effectively discarding 75% of the activations. For the 3D CNN implementation, a filter size of  2 &times; 2 &times; 2  and a stride of 2 will be used in each max pooling layer following the convolutional layers.
          </p>
          <h4>Dense Layers + Added Features</h4>
          <p>Dense layers come after the series of convolutional layers, mapping the outputs of those convolutions to a final classification using one or more hidden layers. These layers, also known as fully-connected layers, consist of neurons with weights and biases as seen in a multilayer perceptron, with each neuron in a layer routed to every neuron in the next layer. Each neuron uses its inputs, weights, bias, and activation function to pass information to the next layer, where the output <i>y</i> is calculated with the equation <p align=center><a href="https://www.codecogs.com/eqnedit.php?latex=y=f\left&space;(&space;\sum_{i=0}^{n}x_{i}w_{i}&plus;b&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=f\left&space;(&space;\sum_{i=0}^{n}x_{i}w_{i}&plus;b&space;\right&space;)" title="y=f\left ( \sum_{i=0}^{n}x_{i}w_{i}+b \right )" />
          </a></p> for inputs <i>x</i><sub>1:n</sub>, weights <i>w</i><sub>1:n</sub>, bias parameter <i>b</i>, and activation function <i>f</i>. The weights and biases are then updated with gradient descent during backpropagation. At a higher level, the convolutional layers can be thought of as a feature extraction system, while the dense layers analyze those features to model the underlying relationships.
          </p>
          <h4>Batch Normalization</h4>
          <p>Batch Normalization is a technique developed to address the problem of internal covariate shift, in which training of deep learning models becomes less efficient due to the need to adapt to new input distributions as parameters are changed. Batch Normalization makes normalization a part of the model architecture, applying the operation for each training mini-batch and conforming the activations at each point in the network to a unit gaussian distribution. There is some debate as to whether the effectiveness of Batch Normalization is a result of reducing internal covariate shift or if there are more significant impacts, such as the smoothening of the optimization landscape, which create more stable and traversable gradients. Nevertheless, the use of mini-batch statistics for normalization has been proven to improve the training process, not only in the efficiency of convergence during training, but also through regularizing properties as a result of the influence of the whole mini-batch on each individual training element, and is now widely used in practice. In the original paper, Batch Normalization was designed to be used in between the fully connected or convolutional layers and their activations, but there is some confusion as to whether better results are achieved with Batch Normalization used after the non-linearity. We achieved slightly better results with the use of Batch Normalization before the activation functions. We will use Batch Normalization following each convolutional layer.
          </p>
          <br>
          <img style="height: 75%; width: 75%; display: block; margin-left: auto; margin-right: auto;" src="PD-CNN Model.jpg" alt="image error">
          <p style="font-size:12px; text-align:center; margin-left: 90px; margin-right: 90px;">Architecture of 3D Convolutional Neural Network: L<sub>1,4,7</sub>: 3D convolutional layers; L<sub>2,5,8</sub>: Batch Normalization; L<sub>3,6,9</sub>: max pooling; L<sub>10,11,12</sub>: dense layers; L<sub>13</sub>: output layer. (A) Full CNN framework without added features. (B) Simplified CNN framework demonstrating implementation of added features.
          </p>
      </section>
      <hr>
      <section style="padding-top:40px; margin-top:-50px;" id="sec3">
        <h3>Preprocessing</h3>
        <p>In this section, we will describe the preprocessing steps taken before using the data in the 3D Convolutional Neural Network. The preprocessing was done using Jupyter Notebook on files downloaded from the PPMI database containing DaTscan SPECT images. Unfortunately, the PPMI data is for research purposes and can only be acquired by request, but the ideas in this section can still be applied to other datasets.
        <br><br>The libraries needed for our purposes are shown here:
        </p>
        <script src="https://gist.github.com/govindramesh/0332eb7093ef2d52da15a545884a9ee9.js"></script>
        <p><br>We use os.walk and pydicom to read and append each DICOM (.dcm) file containing the 3D DaTscan SPECT images to a list, ‘scans’. For Macs, we also need to make sure we don’t pick up any .DS_Store hidden files. <i>Note: make sure the files are ordered by class before extracting them.</i>
        </p>
        <script src="https://gist.github.com/govindramesh/3baae659ee8449b3cc813f09e04b5426.js"></script>
        <p><br>DICOM files contain more than just the DaTscan SPECT images. For our purposes we only need the 3D arrays representing the DaTscan SPECT images, so we extract them to a list.
        </p>
        <script src="https://gist.github.com/govindramesh/bd0df125deeb1247f9225d3553b22358.js"></script>
        <p><br>Here, we perform a common type of preprocessing, mean subtraction, by taking the mean of all the 3D images and subtracting it from each individual image. As a result, each voxel position in the dataset is centered around zero. With mean subtraction, the CNN will be able to learn more efficiently during backpropagation without imbalanced gradients throughout the training data.
        </p>
        <script src="https://gist.github.com/govindramesh/bd063601dddb614004164782986c250b.js"></script>
        <p><br>The next stage of preprocessing we perform is feature reduction. This is done with a thresholding process in which a binary mask is created from the average of each 3D image in the dataset by an intensity threshold. The intensity threshold is a percentage of the maximum value of the average 3D image such that any value above the threshold will compose the binary contour mask. From the binary contour mask, the maximum and minimum indices of the mask for each dimension are used to create a rectangular 3D mask, which will then be applied to every DaTscan SPECT image.
        </p>
        <script src="https://gist.github.com/govindramesh/10b60675dd0013bde868e629f92f5b88.js"></script>
        <p><br>We need to add a 4th dimension to our images, representing how many color channels the image has, in order to use them in a 3D CNN. We are using grayscale images, so our 4th dimension will be 1. Then, we can export the images as a NumPy array file (.npy), separated by class.
        </p>
        <script src="https://gist.github.com/govindramesh/783f28a95382cf1479dee8d4ca11270d.js"></script>
      </section>
      <hr>
      <section style="padding-top:40px; margin-top:-50px;" id="sec4">
        <h3>3D Convolutional Neural Network Code</h3>
        <p>In this section, we will be going over how to implement a basic 3D Convolutional Neural Network using Keras and steps to improve it. All code was run on Google Colaboratory using DaTscan SPECT images from the PPMI database.
        <br><br>The libraries we need to import are shown here:
        </p>
        <script src="https://gist.github.com/govindramesh/ac906254d79c274a72a029e9aedcb7d1.js"></script>
        <p><br>We load the files from the preprocessing stage and append the 3D arrays to a list, along with either a 0 (NC) or 1 (PD) as a label.
        </p>
        <script src="https://gist.github.com/govindramesh/812867f225e89dea803eb67451162b77.js"></script>
        <p><br>We then split the data into a train (90%) and test (10%) set. A simple data augmentation technique is performed on the training set by flipping the image horizontally, effectively doubling the number of samples.
        </p>
        <script src="https://gist.github.com/govindramesh/019b0ecb9941a78bea2ed47fbf753d6f.js"></script>
        <p><br>Here, we set up the hyperparameters for training the model. We start with a validation split of 20%, batch size of 32, learning rate of 0.001, and 25 epochs. An effective way to tune your hyperparameters is through a grid search, however that is out of the scope of this article.
        </p>
        <script src="https://gist.github.com/govindramesh/6361b6109d363f5d4fb46c58e4f30743.js"></script>
        <p><br>To start with, we define a simple model architecture, with two sets of convolution and max pooling layers followed by two dense layers and the output layer.
        </p>
        <script src="https://gist.github.com/govindramesh/6af51bd1613e1357faf530f3fc986b87.js"></script>
        <p><br>We can now create, compile, train and evaluate our model. We are using Adam optimizer for gradient descent. The confusion matrix is produced by scikit-learn.
        </p>
        <script src="https://gist.github.com/govindramesh/4b51f4aea569fe96ec37bcbe95bd6d4c.js"></script>
        <p>We get the following output:
        </p>
        <pre style="font-size: 12px;">
    Epoch 1/25
    36/36 [==============================] - 3s 71ms/step - loss: 9399.1885 - accuracy: 0.8603 - val_loss: 201.6312 - val_accuracy: 0.9617
    Epoch 2/25
    36/36 [==============================] - 2s 66ms/step - loss: 212.4684 - accuracy: 0.9380 - val_loss: 90.0932 - val_accuracy: 0.9582
    Epoch 3/25
    36/36 [==============================] - 2s 66ms/step - loss: 90.1802 - accuracy: 0.9616 - val_loss: 50.8489 - val_accuracy: 0.9652
    Epoch 4/25
    36/36 [==============================] - 2s 66ms/step - loss: 19.0888 - accuracy: 0.9677 - val_loss: 27.7572 - val_accuracy: 0.9756
    Epoch 5/25
    36/36 [==============================] - 2s 66ms/step - loss: 4.0007 - accuracy: 0.9878 - val_loss: 42.5644 - val_accuracy: 0.9617
    Epoch 6/25
    36/36 [==============================] - 2s 66ms/step - loss: 9.7737 - accuracy: 0.9834 - val_loss: 33.9823 - val_accuracy: 0.9686
    Epoch 7/25
    36/36 [==============================] - 2s 66ms/step - loss: 1.8664 - accuracy: 0.9974 - val_loss: 27.3253 - val_accuracy: 0.9756
    Epoch 8/25
    36/36 [==============================] - 2s 66ms/step - loss: 0.7707 - accuracy: 0.9974 - val_loss: 32.8416 - val_accuracy: 0.9721
    Epoch 9/25
    36/36 [==============================] - 2s 67ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.3769 - val_accuracy: 0.9721
    Epoch 10/25
    36/36 [==============================] - 2s 65ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.3667 - val_accuracy: 0.9721
    Epoch 11/25
    36/36 [==============================] - 2s 66ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.3667 - val_accuracy: 0.9721
    ...
    Epoch 24/25
    36/36 [==============================] - 2s 67ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.3667 - val_accuracy: 0.9721
    Epoch 25/25
    36/36 [==============================] - 2s 67ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.3667 - val_accuracy: 0.9721
    Test loss: 72.55394744873047 / Test accuracy: 0.9113923907279968
    [[15  4]
     [ 3 57]]
        </pre>
        <p>We achieved 91.1% accuracy, which is pretty good for the simple model we trained. However, there are multiple steps we can take to improve our results.
          <br><br>Looking at the training and validation loss, we can see that from epochs 9-25, the model stops learning and begins to overfit. This could be a result of our learning rate being set too high or our model training over too many epochs. We can lower the learning rate to 0.00001 and implement early stopping to combat overfitting. Early stopping works by evaluating training metrics such as the loss or accuracy of the validation set at the end of each epoch and ending the training process once there is evidence of overfitting. We use two hyperparameters, patience and minimum &Delta;, while monitoring the validation loss. The patience parameter determines the number of epochs without improvement before training is stopped and minimum &Delta; is used to set the value the validation metric must improve by over the epochs to be considered an improvement and continue training.
        </p>
        <script src="https://gist.github.com/govindramesh/27db67f53abd23bc481eeab85fc7b4ec.js"></script>
        <p>Our loss is also very high, implying that our model weights are using large numbers. This is most likely a result of our data containing voxel values up to 32767. To prevent this, we can implement Batch Normalization between each Conv3D and ReLU Activation layer:
        </p>
        <script src="https://gist.github.com/govindramesh/9cb37653b33a416a420ccb8997621e3e.js"></script>
        <p>With these additional steps, we obtain the following output:
        </p>
        <pre style="font-size: 12px;">
      Epoch 1/25
      36/36 [==============================] - 4s 104ms/step - loss: 2.4551 - accuracy: 0.8428 - val_loss: 6.2586 - val_accuracy: 0.8815
      Epoch 2/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.6320 - accuracy: 0.9293 - val_loss: 0.9931 - val_accuracy: 0.8955
      Epoch 3/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.3781 - accuracy: 0.9188 - val_loss: 0.1550 - val_accuracy: 0.9582
      Epoch 4/25
      36/36 [==============================] - 4s 99ms/step - loss: 0.2046 - accuracy: 0.9397 - val_loss: 0.5437 - val_accuracy: 0.9268
      Epoch 5/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.3989 - accuracy: 0.9450 - val_loss: 0.0936 - val_accuracy: 0.9721
      Epoch 6/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.1592 - accuracy: 0.9485 - val_loss: 0.1018 - val_accuracy: 0.9652
      Epoch 7/25
      36/36 [==============================] - 4s 101ms/step - loss: 0.1718 - accuracy: 0.9633 - val_loss: 0.0620 - val_accuracy: 0.9791
      Epoch 8/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.0876 - accuracy: 0.9764 - val_loss: 0.0744 - val_accuracy: 0.9582
      Epoch 9/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.1265 - accuracy: 0.9642 - val_loss: 0.0615 - val_accuracy: 0.9721
      Epoch 10/25
      36/36 [==============================] - 4s 100ms/step - loss: 0.0747 - accuracy: 0.9799 - val_loss: 0.0647 - val_accuracy: 0.9791
      Test loss: 0.17840932309627533 / Test accuracy: 0.9367088675498962
      [[16  2]
       [ 3 57]]
        </pre>
        <p>Right away we can see an improvement in both the accuracy and loss. Our validation loss is now almost at 0 as a result of smaller weight values and is also much closer to the training loss, which shows that we are no longer overfitting. Our validation loss is actually lower than the training loss for a majority of the epochs, which means our model is underfitting and not performing to its full capacities. In order to utilize more of information in the training data, we can add more convolutional and dense layers to make our model deeper and increase the number of parameters it needs to learn. Our new model is shown here:
        </p>
        <script src="https://gist.github.com/govindramesh/f457514ea144cba5d712c542175c20a8.js"></script>
        <p>And the output for this model:
        </p>
        <pre style="font-size: 12px;">
      Epoch 1/25
      36/36 [==============================] - 4s 114ms/step - loss: 0.5405 - accuracy: 0.7520 - val_loss: 0.3546 - val_accuracy: 0.8153
      Epoch 2/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.2299 - accuracy: 0.9197 - val_loss: 0.1696 - val_accuracy: 0.9338
      Epoch 3/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.1633 - accuracy: 0.9467 - val_loss: 0.1147 - val_accuracy: 0.9582
      Epoch 4/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1320 - accuracy: 0.9563 - val_loss: 0.0933 - val_accuracy: 0.9791
      Epoch 5/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.1183 - accuracy: 0.9624 - val_loss: 0.0800 - val_accuracy: 0.9826
      Epoch 6/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1121 - accuracy: 0.9572 - val_loss: 0.0730 - val_accuracy: 0.9721
      Epoch 7/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.0957 - accuracy: 0.9686 - val_loss: 0.0664 - val_accuracy: 0.9721
      Epoch 8/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.0987 - accuracy: 0.9668 - val_loss: 0.0790 - val_accuracy: 0.9756
      Epoch 9/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.0789 - accuracy: 0.9790 - val_loss: 0.0655 - val_accuracy: 0.9721
      Epoch 10/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.0760 - accuracy: 0.9825 - val_loss: 0.0684 - val_accuracy: 0.9686
      Test loss: 0.11230949312448502 / Test accuracy: 0.9746835231781006
      [[18  1]
       [ 1 59]]</pre>
        <p>We can see the effect of the added depth in our network with our obtained accuracy of 97.5%&mdash;a significant improvement over the other models.
        </p>
        <p><br><span style="font-weight:bold;font-size:22;">Adding Features</span><br>Another step we can take is implementing extra features or covariates in our network. This is done by concatenating the extra features to the features extracted from the images before the dense layers. The procedure to implement these features is outlined below.
          <br><br>In addition to the 3D arrays and labels we used previously, we also need to load in the extra features (in this example, age and sex) from CSV files and append them to the list. The order of the extra features needs to match that of the images.
        </p>
        <script src="https://gist.github.com/govindramesh/e21c2e05d6c296dab498764e61e05e42.js"></script>
        <p><br>Just like the images, we split the features into a train and test set, keeping the features together in a 2 &times; 1 array. Make sure to account for data augmentation.
        </p>
        <script src="https://gist.github.com/govindramesh/f6cf2525685bf8dbf1d3896c448ec882.js"></script>
        <p><br>Using the same hyperparameters, we can now define the new model:
        </p>
        <script src="https://gist.github.com/govindramesh/102d738e84a63750ab97a1effe19fab4.js"></script>
        <p><br>We compile, train, and evaluate our model with some minor changes:
        </p>
        <script src="https://gist.github.com/govindramesh/f7d55b70daa523abbb0d0c36a26329ce.js"></script>
        <p>Obtaining the following result:
        </p>
        <pre style="font-size: 12px;">
      Epoch 1/25
      36/36 [==============================] - 4s 114ms/step - loss: 0.3199 - accuracy: 0.8760 - val_loss: 0.1879 - val_accuracy: 0.9303
      Epoch 2/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1843 - accuracy: 0.9441 - val_loss: 0.1112 - val_accuracy: 0.9686
      Epoch 3/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1464 - accuracy: 0.9432 - val_loss: 0.0943 - val_accuracy: 0.9686
      Epoch 4/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1238 - accuracy: 0.9555 - val_loss: 0.0896 - val_accuracy: 0.9721
      Epoch 5/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.1188 - accuracy: 0.9659 - val_loss: 0.0808 - val_accuracy: 0.9686
      Epoch 6/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.1131 - accuracy: 0.9642 - val_loss: 0.0782 - val_accuracy: 0.9721
      Epoch 7/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0966 - accuracy: 0.9668 - val_loss: 0.0726 - val_accuracy: 0.9686
      Epoch 8/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0935 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9721
      Epoch 9/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0816 - accuracy: 0.9782 - val_loss: 0.0679 - val_accuracy: 0.9756
      Epoch 10/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0763 - accuracy: 0.9764 - val_loss: 0.0704 - val_accuracy: 0.9686
      Epoch 11/25
      36/36 [==============================] - 4s 111ms/step - loss: 0.0738 - accuracy: 0.9782 - val_loss: 0.0621 - val_accuracy: 0.9756
      Epoch 12/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0650 - accuracy: 0.9834 - val_loss: 0.0636 - val_accuracy: 0.9721
      Epoch 13/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0625 - accuracy: 0.9825 - val_loss: 0.0631 - val_accuracy: 0.9721
      Epoch 14/25
      36/36 [==============================] - 4s 110ms/step - loss: 0.0545 - accuracy: 0.9886 - val_loss: 0.0585 - val_accuracy: 0.9791
      Epoch 15/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0472 - accuracy: 0.9860 - val_loss: 0.0603 - val_accuracy: 0.9721
      Epoch 16/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 0.0663 - val_accuracy: 0.9756
      Epoch 17/25
      36/36 [==============================] - 4s 109ms/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 0.0579 - val_accuracy: 0.9756
      Test loss: 0.0918487012386322 / Test accuracy: 0.9746835231781006
      [[18  1]
       [ 1 59]]</pre>
         <p>While there was no improvement in accuracy, the overall loss and fit to the training data was better than without added features. The added features could provide better results when testing on different cuts of the data or when evaluating with K-fold cross validation.
         </p>
      </section>
    </div>

    <!-- Footer Section -->
    <footer></footer>
</body>
</html>
