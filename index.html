<!DOCTYPE html>
<html>
<head>
    <title>Govind Ramesh</title>
    <style>
        .title a {
            display: block;
            font-size:40px;
            padding: 10px 0px 0px 10px;
	    background-color: green;
            color:black;
            font-weight:bold;
            text-align: left;
            text-decoration: none;
        }
        .subtitle a {
            display: block;
            font-size:17px;
	          padding: 0px 0px 10px 10px;
            text-align: left;
            text-decoration: none;
        }
        body {
            margin: 0 auto;
            background-position:center;
            background-size: contain;
        }

        .menu {
            position: sticky;
            top: 0;
            background-color: black;
            padding:10px 0px 10px 0px;
            color:white;
            margin: 0 auto;
            overflow: hidden;
        }
        .menu a {
            color: white;
            text-align: center;
            padding: 14px 16px;
            text-decoration: none;
            font-size: 20px;
        }
        footer {
            width: 100%;
            bottom: 0px;
            background-color: black;
            color: white;
            position: fixed;
	          padding:10px 0px 10px 0px;
            text-align:center;
            font-size:17px;
            font-weight:bold;
        }
        .body_sec {
            margin-left:30px;
            margin-right:30px;
            margin-bottom:60px;
            font-size:17px;
        }
    </style>
</head>

<body>

    <!-- Header Section -->
    <header>
        <div class = "title"><a href="https://govindramesh.github.io">Govind Ramesh</a></title>
        <div class="subtitle"><a href="https://govindramesh.github.io">GitHub Page for Programming Projects</a></div>
    </header>

    <!-- Menu Navigation Bar -->
    <div class="menu">
        <a href="https://govindramesh.github.io">HOME</a>
        <a href="https://govindramesh.github.io/PD-3DCNN/">PD-3DCNN</a>
        <a href="https://govindramesh.github.io/PoliceBrutality/">Police Brutality</a>
    </div>

    <!-- Body section -->
    <div class = "body_sec">
      <section id="Intro">
          <h3>Content section</h3>
           <a target="_blank" rel="noopener noreferrer" href="Research Paper.pdf">Link to pdf</a>
           <p>In this post, we will be going over what a 3D Convolutional Neural Network is and its applications, how it works, and how to implement one using Keras.
           </p>
      </section>
      <section id="What is CNN?">
          <h2>What is a Convolutional Neural Network?</h2>
          <p>Convolutional Neural Networks (CNNs) are a class of Artificial Neural Networks (ANNs) primarily used for problems involving image classification. Rather than using each pixel (or voxel in 3D) value as a feature as would happen with a standard fully-connected multilayer perceptron, CNNs connect each neuron to a small section of the prior layer, drastically reducing the number of parameters and, as a result, the amount of overfitting and computational power. This process is done through the use of learnable filters and convolutions, hence the name CNN.
          </p>
          <p>Convolutional Neural Networks have become increasingly popular in the medical field due to their effectiveness in image analysis. They have been used throughout literature for tasks such as image segmentation, disease classification, abnormality detection, and computer aided diagnosis. For our purposes, we will be using a 3D Convolutional Neural Network to diagnose Parkinson’s Disease (PD) using 871 DaTscan SPECT brain images from the PPMI database.
          </p>
      </section>
      <section id="CNN Architecture">
          <h2>Convolutional Neural Network Architecture</h2>
          <p>A standard Convolutional Neural Network architecture consists of convolutional layers, pooling layers, and dense layers. In general, the input images go through a series of convolutional layers and a pooling of the activations, followed by dense layers which make the classification. There are many variations of CNNs currently in practice such as AlexNet, ResNet, GoogLeNet, and VGGNet. The learning process is done through backpropagation, in which the gradient of the error function is calculated with respect to the network’s weights. Through gradient descent, the error is minimized by updating the weights.
          </p>
          <h4>3D Convolutional Layer</h4>
          <p>The convolutional layer is central to the function of CNNs, using a set of learnable filters that function similar to weights in a multilayer perceptron. These filters are relatively small compared to the image. During the forward pass, each filter shifts, or convolves, along the dimensions of the input image, computing dot products in which each element in the filter is multiplied by its respective value in the filter-sized patch of the input image and summed. These dot products are arranged spatially, producing an activation map of the image showing where the filter was detected in the input image. Through this process, convolutional layers introduce translation invariance, since the shifting of the filter across the whole input enables the identification of that feature anywhere in the image. The values within each filter are learned and updated during backpropagation with gradient descent to minimize the loss.
          </p>
          <p>In a 2D convolutional layer, the activation maps produced are 2D, but are computed multiple times for the total number of filters in the layer (a hyperparameter) and stacked to create a 3D output volume. Therefore, extending to a 3D convolutional layer, a number of 3D filters convolve along the dimensions of the input volume, creating 3D activation maps which are stacked to create a 4D volume, in which the 4th dimension is the number of filters in the layer.
          </p>
          <p>Convolutional layers can be applied to not only the input image, but also the outputs of other convolutional layers (following activation and pooling operations). By stacking multiple convolutional layers, deeper layers are able to detect more complex features. Initial convolutional layers can learn simple features such as lines from the input image, while subsequent layers can extract combinations of these simple features to construct more complex filters specific to the task.
          </p>
          <p>Convolutional layers use four main hyperparameters: filter size, number of filters (<i>n</i>), stride, and zero-padding. Filter size in a 2D CNN is generally set to be a square with size of 3, 5, or 7. For our purposes, we will use filter dimensions of 3 &times; 3 &times; 3 for each 3D convolutional layer. We use three convolutional layers, progressively increasing the number of filters for each subsequent layer, resulting in <i>n</i><sub>1</sub> &equals; 32, <i>n</i><sub>2</sub> &equals; 64, and <i>n</i><sub>3</sub> &equals; 128 filters. Stride determines how many pixels or voxels the filter shifts over for each convolution, and is usually set to 1 since any higher value reduces the amount of overlap and downsamples the input, an operation that is already performed with pooling. Zero-padding adds a border of zeros around the input, enabling the voxels on the edges to be in the center of the filter during convolution and creating an output activation map with the same dimensions as the input. We use zero-padding in each convolutional layer.
          </p>
          <h4>Activation</h4>
          <p>Activation functions are applied to every output function in ANNs. Activations enable the model to learn complex patterns in multidimensional data by introducing non-linearity. There are many activation functions, but the most popular in recent years is the Rectified Linear Unit (ReLU), especially for CNNs. ReLU functions avoid the problem of saturation seen with Tanh and Sigmoid activations, which effectively kills the gradient and the learning process. Hence, ReLU has been shown to converge six times faster than other activation functions. ReLU is computed by the function <p align = center><a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=max(0,x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=max(0,x)" title="f(x)=max(0,x)" />
          </a></p> which proves to be computationally efficient during backpropagation compared to other functions using expensive operations. In this study, we will use the ReLU activation function with every convolutional and dense layer.
          </p>
          <h4>Max Pooling</h4>
          <p>Pooling layers serve as a downsampling technique used between convolutional layers. Pooling reduces the amount of parameters which decreases the amount of computation power needed and limits overfitting to the data. In practice, max pooling layers are used, in which the max operation is applied to the pool. Traditional max pooling layers use filters/pool size of dimensions 2 &times; 2 and a stride of 2 for the downsampling operation, effectively discarding 75% of the activations. For the 3D CNN implementation, a filter size of  2 &times; 2 &times; 2  and a stride of 2 will be used in each max pooling layer following the convolutional layers.
          </p>
          <h4>Dense Layers + Added Features</h4>
          <p>Dense layers come after the series of convolutional layers, mapping the outputs of those convolutions to a final classification using one or more hidden layers. These layers, also known as fully-connected layers, consist of neurons with weights and biases as seen in a multilayer perceptron, with each neuron in a layer routed to every neuron in the next layer. Each neuron uses its inputs, weights, bias, and activation function to pass information to the next layer, where the output <i>y</i> is calculated with the equation <p align=center><a href="https://www.codecogs.com/eqnedit.php?latex=y=f\left&space;(&space;\sum_{i=0}^{n}x_{i}w_{i}&plus;b&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=f\left&space;(&space;\sum_{i=0}^{n}x_{i}w_{i}&plus;b&space;\right&space;)" title="y=f\left ( \sum_{i=0}^{n}x_{i}w_{i}+b \right )" />
          </a></p> for inputs <i>x</i><sub>1:n</sub>, weights <i>w</i><sub>1:n</sub>, bias parameter <i>b</i>, and activation function <i>f</i>. The weights and biases are then updated with gradient descent during backpropagation. At a higher level, the convolutional layers can be thought of as a feature extraction system, while the dense layers analyze those features to model the underlying relationships.
          </p>
          <h4>Batch Normalization</h4>
          <p>Batch Normalization is a technique developed to address the problem of internal covariate shift, in which training of deep learning models becomes less efficient due to the need to adapt to new input distributions as parameters are changed. Batch Normalization makes normalization a part of the model architecture, applying the operation for each training mini-batch and conforming the activations at each point in the network to a unit gaussian distribution. There is some debate as to whether the effectiveness of Batch Normalization is a result of reducing internal covariate shift or if there are more significant impacts, such as the smoothening of the optimization landscape, which create more stable and traversable gradients. Nevertheless, the use of mini-batch statistics for normalization has been proven to improve the training process, not only in the efficiency of convergence during training, but also through regularizing properties as a result of the influence of the whole mini-batch on each individual training element, and is now widely used in practice. In the original paper, Batch Normalization was designed to be used in between the fully connected or convolutional layers and their activations, but there is some confusion as to whether better results are achieved with Batch Normalization used after the non-linearity. We achieved slightly better results with the use of Batch Normalization before the activation functions. We will use Batch Normalization following each convolutional layer.
          </p>
      </section>
    </div>

    <!-- Footer Section -->
    <footer></footer>
</body>
</html>
